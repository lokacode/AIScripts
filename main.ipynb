{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebooks for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and login to huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hf_transfer -q\n",
    "from huggingface_hub import login\n",
    "login(\"your HF token here\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bloke AIScripts clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /content/ ; git clone https://github.com/TheBlokeAI/AIScripts.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ExLlamaV2 Quantization](https://github.com/turboderp/exllamav2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ExLlamaV2 git clone + requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/turboderp/exllamav2\n",
    "%cd /content/exllamav2\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /content/exllamav2/ ;mkdir dataset\n",
    "\n",
    "# WikiText Dataset\n",
    "# !cd /content/exllamav2/datase ;wget https://huggingface.co/datasets/wikitext/resolve/refs%2Fconvert%2Fparquet/wikitext-2-v1/test/0000.parquet\n",
    "\n",
    "# Roleplay Dataset\n",
    "!cd /content/exllamav2/datase ;wget https://huggingface.co/datasets/royallab/PIPPA-cleaned/resolve/main/pippa_raw_fix.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Write the name of the model you want to download from the HuggingFace Hub\n",
    "MODEL_ID = \"Gryphe/MythoMax-L2-13b\" #@param {type:\"string\"}\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "!cd AIScripts ;python hub_download.py --symlinks false $MODEL_ID \"/content/\"$MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model quantization\n",
    "\n",
    "### Arguments\n",
    "\n",
    "Here are the arguments to `convert.py`:\n",
    "\n",
    "- **-i / --in_dir *directory***: _(required)_ The source model to convert, in HF format (FP16). The directory should \n",
    "contain at least a `config.json` file, a `tokenizer.model` file and one or more `.safetensors` files containing weights.\n",
    "If there are multiple weights files, they will all be indexed and searched for the neccessary tensors, so sharded models are \n",
    "supported.\n",
    "  \n",
    "\n",
    "- **-o / --out_dir *directory***: _(required)_ A working directory where the converter can store temporary files and deposit\n",
    "the final output unless **-cf** is also provided. If this directory is not empty when the conversion starts, the converter\n",
    "will attempt to resume whatever was being worked on in that directory. So if a job is interrupted, you can rerun the\n",
    "script with the same arguments to pick up from where it stopped. Note that parameters are read from the `job.json`\n",
    "file kept in this directory, so you won't be able to supply new parameters to a resumed job without editing that file.\n",
    "If you do, note that not all changes would leave the job in a valid state, so be careful with that.\n",
    "  \n",
    "  \n",
    "- **-nr / --no_resume**: If this flag is specified, the converter will not resume an interrupted job even if you point it\n",
    "to a non-empty working directory. If the working directory is not empty, all files within it will be deleted and the\n",
    "converter will start a new job.\n",
    "  \n",
    "  \n",
    "- **-cf / --compile_full *directory***: By default the resulting `.safetensors` files are saved to the working directory.\n",
    "If you specify a directory with **-cf**, the quantized weights will be saved there instead. In addition, all files from\n",
    "the model (input) directory will be copied to this directory except for the original `.safetensors` files, resulting in\n",
    "a full model directory that can be used for inference by ExLlamaV2.\n",
    "  \n",
    "\n",
    "- **-om / --output_measurement *file***: After the first (measurement) pass is completed, a `measurement.json` file will\n",
    "be dropped in the working directory (**-o**). If you specify **-om** with a path, the measurement will be saved to that\n",
    "path after the measurement pass, and the script will exit immediately after.\n",
    "  \n",
    "\n",
    "- **-m / --measurement *file***: Skip the measurement pass and instead use the results from the provided file. This is\n",
    "particularly useful when quantizing the same model to multiple bitrates, since the measurement pass can take a long time\n",
    "to complete.\n",
    "  \n",
    "\n",
    "- **-c / --cal_dataset *file***: (_optional_) The calibration dataset in Parquet format. The quantizer concatenates all\n",
    "the data in this file into one long string and uses the first _r_ \\* _l_ tokens for calibration. If this is not\n",
    "specified, the default, built-in calibration dataset is used which contains a broad mix of different types of data. It's\n",
    "designed to prevent the quantized model from overfitting to any particular mode, language or style, and generally\n",
    "results in more robust, reliable outputs, especially at lower bitrates.\n",
    "  \n",
    "\n",
    "- **-l / --length *int***: Length, in tokens, of each calibration row. Default is 2048.\n",
    "  \n",
    "\n",
    "- **-r / --dataset_rows *int***: Number of rows in the calibration batch. Default is 100.\n",
    "  \n",
    "\n",
    "- **-ml / --measurement_length *int***: Length, in tokens, of each calibration row used for the measuring pass. Default\n",
    "is 2048. \n",
    "  \n",
    "\n",
    "- **-mr / --measurement_rows *int***: Number of rows in the calibration batch for the measuring pass. Default is 16.\n",
    "  \n",
    "\n",
    "- **-b / --bits *float***: Target average number of bits per weight.\n",
    "  \n",
    "\n",
    "- **-hb / --bits *int***: Number of bits for the lm_head (output) layer of the model. Default is 6, although that\n",
    "value actually results in a mixed-precision quantization of about 6.3 bits. Options are 2, 3, 4, 5, 6 and 8. (Only 6\n",
    "and 8 appear to be useful.)\n",
    "\n",
    "  \n",
    "- **-ss / --shard_size *float***: Output shard size, in megabytes. Default is 8192. Set this to 0 to disable sharding.\n",
    "Note that writing a very large `.safetensors` file can require a lot of system RAM.\n",
    "\n",
    "\n",
    "- **-ra / --rope_alpha *float***: RoPE (NTK) alpha to apply to base model for calibration.\n",
    "\n",
    "\n",
    "- **-rs / --rope_scale *float***: RoPE scaling factor to apply to base model for calibration. This settings is not \n",
    "automatically read from the model's config, so it's strongly recommended that you check what setting the model was\n",
    "trained/finetuned with. E.g.: deepseek-coder uses a scaling factor of 4, so will be incorrectly calibrated if you\n",
    "convert it without `-rs 4`.\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "The converter works in two passes; first it measures how quantization impacts each module of the model, and then it\n",
    "actually quantizes the model, choosing quantization parameters for each layer that minimize the overall error while \n",
    "also achieving the desired overall (average) bitrate.\n",
    "\n",
    "The first pass is slow, since it effectively quantizes the entire model about 12 times over (albeit with a less\n",
    "comprehensive sample of the calibration dataset), so make sure to save the `measurement.json` file so you can skip the\n",
    "measurement pass on subsequent quants of the same model.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Convert a model and create a directory containing the quantized version with all of its original files:\n",
    "\n",
    "```\n",
    "python convert.py \\\n",
    "    -i /mnt/models/llama2-7b-fp16/ \\\n",
    "    -o /mnt/temp/exl2/ \\\n",
    "    -cf /mnt/models/llama2-7b-exl2/3.0bpw/ \\\n",
    "    -b 3.0 \n",
    "```\n",
    "\n",
    "Run just the measurement pass on a model, clearing the working directory first:\n",
    "\n",
    "```\n",
    "python convert.py \\\n",
    "    -i /mnt/models/llama2-7b-fp16/ \\\n",
    "    -o /mnt/temp/exl2/ \\\n",
    "    -nr \\\n",
    "    -om /mnt/models/llama2-7b-exl2/measurement.json\n",
    "```\n",
    "\n",
    "Use that measurement to quantize the model at two different bitrates:\n",
    "\n",
    "```\n",
    "python convert.py \\\n",
    "    -i /mnt/models/llama2-7b-fp16/ \\\n",
    "    -o /mnt/temp/exl2/ \\\n",
    "    -nr \\\n",
    "    -m /mnt/models/llama2-7b-exl2/measurement.json \\\n",
    "    -cf /mnt/models/llama2-7b-exl2/4.0bpw/ \\\n",
    "    -b 4.0\n",
    "    \n",
    "python convert.py \\\n",
    "    -i /mnt/models/llama2-7b-fp16/ \\\n",
    "    -o /mnt/temp/exl2/ \\\n",
    "    -nr \\\n",
    "    -m /mnt/models/llama2-7b-exl2/measurement.json \\\n",
    "    -cf /mnt/models/llama2-7b-exl2/4.5bpw/ \\\n",
    "    -b 4.5\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "- If the conversion script seems to stop on the \"Solving...\" step, give it a moment. It's attempting to find the \n",
    "combination of quantization parameters within the bits budget that minimizes the product of measured errors per\n",
    "individual layer, and the implementation is not very efficient.\n",
    "- During measurement and conversion of MoE models you may see a message like: \n",
    "`!! Warning: w2.7 has less than 10% calibration for 77/115 rows`. This happens when a particular expert isn't triggered\n",
    "enough during the reference forward passes to get a good amount of calibration data. It won't cause the\n",
    "conversion to fail, and it may not be a big deal at all, but GPTQ-style quantization of MoE models is very new so I'm\n",
    "not yet sure if it actually matters.\n",
    "- After conversion, the \"calibration perplexity (quant)\" is a perplexity calculation on a small sample of the \n",
    "calibration data as processed by the quantized model under construction. If it looks too high (30 or more), \n",
    "quantization likely didn't go well, and if it's unreasonably high (in the thousands, for instance) quantization failed\n",
    "catastrophically. \n",
    "\n",
    "### Hardware requirements\n",
    "\n",
    "Roughly speaking, you'll need about 64 GB or RAM and 24 GB of VRAM to convert a 70B model, while 7B seems to require\n",
    "about 16 GB of RAM and about 8 GB of VRAM.\n",
    "\n",
    "The deciding factor for the memory requirement is the *width* of the model rather than the depth, so 120B models that\n",
    "have the same hidden size as 70B models have the same hardware requirements. Mixtral 8x7B has much wider feed-forward\n",
    "layers so requires about 20 GB of VRAM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title EXL2 Quants\n",
    "%cd /content/exllamav2/\n",
    "!rm -r /content/EXL2\n",
    "!rm -r /content/output\n",
    "!git pull --rebase --autostash\n",
    "%cd /content\n",
    "!mkdir EXL2\n",
    "!mkdir output\n",
    "%cd /content/exllamav2/\n",
    "!python convert.py \\\n",
    "    -i \"/content/MythoMax\" \\\n",
    "    -o \"/content/EXL2\" \\\n",
    "    -c dataset \\\n",
    "    -cf \"/content/output\" \\\n",
    "    -b 6.13 \\\n",
    "    -ss 0 \\\n",
    "    -hb 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/exllamav2/\n",
    "# Test Perplexity\n",
    "!python test_inference.py -m \"/content/time\" -ed \"/content/exllamav2/dataset\"\n",
    "\n",
    "# Test Inference speed\n",
    "!python test_inference.py -m \"/content/models/new_lengkuas\" -ps -s\n",
    "\n",
    "# Test prompt completion\n",
    "!python test_inference.py -m \"/content/Rendang-exl2\" -p \"Once upon a time,\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Upload the model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd AIScripts ;python hub_upload.py \"Your/Repo\" \"Your model directory\"\n",
    "\n",
    "# for example\n",
    "# !cd AIScripts ;python hub_upload.py \"R136a1/NewModel-exl2\" \"/content/NewModel-exl2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama.cpp\n",
    "\n",
    "## Usage\n",
    "\n",
    "* `MODEL_ID`: The ID of the model to quantize (e.g., `mlabonne/EvolCodeLlama-7b`).\n",
    "* `QUANTIZATION_METHOD`: The quantization method to use.\n",
    "\n",
    "## Quantization methods\n",
    "\n",
    "* `q2_k`: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\n",
    "* `q3_k_l`: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
    "* `q3_k_m`: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
    "* `q3_k_s`: Uses Q3_K for all tensors\n",
    "* `q4_0`: Original quant method, 4-bit.\n",
    "* `q4_1`: Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\n",
    "* `q4_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
    "* `q4_k_s`: Uses Q4_K for all tensors\n",
    "* `q5_0`: Higher accuracy, higher resource usage and slower inference.\n",
    "* `q5_1`: Even higher accuracy, resource usage and slower inference.\n",
    "* `q5_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
    "* `q5_k_s`:  Uses Q5_K for all tensors\n",
    "* `q6_k`: Uses Q8_K for all tensors\n",
    "* `q8_0`: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n",
    "\n",
    "As a rule of thumb, **I recommend using Q5_K_M** as it preserves most of the model's performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2_K or Q3_* versions, as they drastically decrease model performance. T4 GPUs (Free on Colab) can handle Q5_K_M models, up to Q6_K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the model + Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "MODEL_ID = \"R136a1/Lengkuas\"\n",
    "QUANTIZATION_METHODS = [\"q_k_m\",\"q6_k\"]\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Download the model\n",
    "%cd /content/\n",
    "!cd AIScripts ;python hub_download.py --symlinks false $MODEL_ID \"/content/\"$MODEL_NAME\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
    "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
    "\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    !./llama.cpp/quantize {fp16} {qtype} {method}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test the model\n",
    "\n",
    "You should replace the directory with the directory of the model you quantized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./llama.cpp/main -ngl 100 -m /content/Lengkuas/lengkuas.Q6_K.gguf --prompt \"Once upon a time,\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Upload the model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd AIScripts ;python hub_upload.py --branch gguf \"R136a1/Madang\" \"/content/Lengkuas/lengkuas.Q6_K.gguf\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
